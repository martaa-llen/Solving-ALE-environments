{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUdrXQCrOkFe",
        "outputId": "b915aef1-e8ec-4d75-d1b2-8510ef8ac508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/958.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py\n",
            "Successfully installed ale-py-0.10.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install gymnasium\n",
        "#!pip install ale-py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GMZ-c7kpN3Jy"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import ale_py\n",
        "from PIL import Image\n",
        "import os\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PJHBECWMN5vM"
      },
      "outputs": [],
      "source": [
        "#DQN model (same as in training)\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AVeoEQEtN77M"
      },
      "outputs": [],
      "source": [
        "def preprocess_observation(observation):\n",
        "    processed_obs = np.mean(observation, axis=2).astype(np.float32)\n",
        "    processed_obs = torch.FloatTensor(processed_obs)\n",
        "    return torch.nn.functional.interpolate(processed_obs.unsqueeze(0).unsqueeze(0),\n",
        "                                         size=(84, 84),\n",
        "                                         mode='bilinear',\n",
        "                                         align_corners=False).squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2gmIYUAOOAI9"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model_path, num_episodes=100):\n",
        "    #init env\n",
        "    env = gym.make(\"ALE/Bowling-v5\", render_mode=\"rgb_array\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #load model\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model = DQN((1, 84, 84), env.action_space.n).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    #metrics storage\n",
        "    episode_rewards = []\n",
        "    best_reward = float('-inf')\n",
        "    best_episode_frames = []\n",
        "\n",
        "    print(f\"Starting evaluation over {num_episodes} episodes...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()[0]\n",
        "        state = preprocess_observation(state).unsqueeze(0).to(device)\n",
        "        episode_reward = 0\n",
        "        frames = []\n",
        "\n",
        "        while True:\n",
        "            #get action \n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = q_values.max(1)[1].view(1, 1)\n",
        "\n",
        "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            episode_reward += reward\n",
        "\n",
        "            #save frame\n",
        "            frames.append(Image.fromarray(env.render()))\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "            #preprocess next state\n",
        "            state = preprocess_observation(observation).unsqueeze(0).to(device)\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        #update best episode if current is better\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            best_episode_frames = frames.copy()\n",
        "\n",
        "        print(f\"Episode {episode + 1}/{num_episodes}\")\n",
        "        print(f\"Reward: {episode_reward}\")\n",
        "        print(f\"Best Reward so far: {best_reward}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    #save best episode as GIF\n",
        "    if best_episode_frames:\n",
        "        os.makedirs(\"evaluation_results\", exist_ok=True)\n",
        "        best_episode_path = \"evaluation_results/best_episode.gif\"\n",
        "        best_episode_frames[0].save(\n",
        "            best_episode_path,\n",
        "            save_all=True,\n",
        "            append_images=best_episode_frames[1:],\n",
        "            duration=100,\n",
        "            loop=0\n",
        "        )\n",
        "        print(f\"Best episode saved to {best_episode_path}\")\n",
        "\n",
        "    #summary statistics\n",
        "    print(\"\\nEvaluation Summary:\")\n",
        "    print(f\"Number of episodes: {num_episodes}\")\n",
        "    print(f\"Mean reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
        "    print(f\"Min reward: {min(episode_rewards):.2f}\")\n",
        "    print(f\"Max reward: {max(episode_rewards):.2f}\")\n",
        "\n",
        "    #log to wandb\n",
        "    wandb.init(project=\"bowling-ale\", name=\"model_evaluation\")\n",
        "    wandb.log({\n",
        "        \"mean_reward\": np.mean(episode_rewards),\n",
        "        \"std_reward\": np.std(episode_rewards),\n",
        "        \"min_reward\": min(episode_rewards),\n",
        "        \"max_reward\": max(episode_rewards),\n",
        "        \"best_episode\": wandb.Video(best_episode_path, fps=30, format=\"gif\")\n",
        "    })\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7q4qbv57NykH",
        "outputId": "2e429ad2-cc2a-417c-bf0d-249fb088f8b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-225d027cacb1>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation over 100 episodes...\n",
            "Episode 1/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 2/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 3/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 4/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 5/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 6/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 7/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 8/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 9/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 10/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 11/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 12/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 13/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 14/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 15/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 16/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 17/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 14.0\n",
            "--------------------------------------------------\n",
            "Episode 18/100\n",
            "Reward: 15.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 19/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 20/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 21/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 22/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 23/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 24/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 25/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 26/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 27/100\n",
            "Reward: 15.0\n",
            "Best Reward so far: 15.0\n",
            "--------------------------------------------------\n",
            "Episode 28/100\n",
            "Reward: 22.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 29/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 30/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 31/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 32/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 33/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 34/100\n",
            "Reward: 11.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 35/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 36/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 37/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 38/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 39/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 40/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 41/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 42/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 22.0\n",
            "--------------------------------------------------\n",
            "Episode 43/100\n",
            "Reward: 24.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 44/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 45/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 46/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 47/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 48/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 49/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 50/100\n",
            "Reward: 11.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 51/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 52/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 53/100\n",
            "Reward: 16.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 54/100\n",
            "Reward: 9.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 55/100\n",
            "Reward: 22.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 56/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 57/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 58/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 59/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 60/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 61/100\n",
            "Reward: 22.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 62/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 63/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 64/100\n",
            "Reward: 22.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 65/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 66/100\n",
            "Reward: 15.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 67/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 68/100\n",
            "Reward: 11.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 69/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 70/100\n",
            "Reward: 22.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 71/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 72/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 73/100\n",
            "Reward: 15.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 74/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 75/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 76/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 77/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 78/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 79/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 80/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 81/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 82/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 83/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 84/100\n",
            "Reward: 9.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 85/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 86/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 87/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 88/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 89/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 90/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 91/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 92/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 93/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 94/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 95/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 96/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 97/100\n",
            "Reward: 11.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 98/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 99/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Episode 100/100\n",
            "Reward: 14.0\n",
            "Best Reward so far: 24.0\n",
            "--------------------------------------------------\n",
            "Best episode saved to evaluation_results/best_episode2.gif\n",
            "\n",
            "Evaluation Summary:\n",
            "Number of episodes: 100\n",
            "Mean reward: 14.34 ± 2.24\n",
            "Min reward: 9.00\n",
            "Max reward: 24.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241215_222229-a6ef1c5k</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale/runs/a6ef1c5k' target=\"_blank\">model_evaluation</a></strong> to <a href='https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale' target=\"_blank\">https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale/runs/a6ef1c5k' target=\"_blank\">https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale/runs/a6ef1c5k</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>max_reward</td><td>▁</td></tr><tr><td>mean_reward</td><td>▁</td></tr><tr><td>min_reward</td><td>▁</td></tr><tr><td>std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>max_reward</td><td>24</td></tr><tr><td>mean_reward</td><td>14.34</td></tr><tr><td>min_reward</td><td>9</td></tr><tr><td>std_reward</td><td>2.24152</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">model_evaluation</strong> at: <a href='https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale/runs/a6ef1c5k' target=\"_blank\">https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale/runs/a6ef1c5k</a><br/> View project at: <a href='https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale' target=\"_blank\">https://wandb.ai/1636312-universitat-aut-noma-de-barcelona/bowling-ale</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241215_222229-a6ef1c5k/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gym.register_envs(ale_py)\n",
        "\n",
        "model_path = \"./Solving-ALE-environments/Part 1 - Bowling/DQN/models_dqn/best_model_dqn.pth\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "evaluate_model(model_path, num_episodes=100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
